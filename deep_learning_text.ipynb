{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Text\n",
    "\n",
    "We will be constructing a bunch of models and see how well they perform on text classification tasks.\n",
    "Let's start with imports and data and do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/congress_train.csv', encoding = \"ISO-8859-1\").dropna()\n",
    "valid_df = pd.read_csv('data/congress_val.csv',encoding = \"ISO-8859-1\").dropna()\n",
    "test_df = pd.read_csv('data/congress_test.csv',encoding = \"ISO-8859-1\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = list(train_df['Title'])\n",
    "train_text = [str(i) for i in train_text]\n",
    "valid_text = list(valid_df['Title'])\n",
    "valid_text = [str(i) for i in valid_text]\n",
    "test_text = list(test_df['Title'])\n",
    "test_text = [str(i) for i in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(list(train_df['Major']))\n",
    "valid_y = to_categorical(list(valid_df['Major']))\n",
    "test_y = to_categorical(list(test_df['Major']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(train_text)\n",
    "test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "valid_seq = tokenizer.texts_to_sequences(valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pad_sequences(train_seq, maxlen=100)\n",
    "test_x = pad_sequences(test_seq, maxlen=100)\n",
    "valid_x = pad_sequences(valid_seq, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up our data, we can start by setting up our imports for our model and create our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a standard feed forward network with embeddings with 128 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward.add(Embedding(10000, 128, input_length=100))\n",
    "feedforward.add(Flatten())\n",
    "feedforward.add(Dense(24, activation='softmax'))\n",
    "feedforward.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 1.2890 - acc: 0.6538 - val_loss: 0.7000 - val_acc: 0.8138\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 54s 192us/step - loss: 0.5994 - acc: 0.8359 - val_loss: 0.6017 - val_acc: 0.8385\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 54s 193us/step - loss: 0.5018 - acc: 0.8609 - val_loss: 0.5696 - val_acc: 0.8473\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 61s 220us/step - loss: 0.4429 - acc: 0.8767 - val_loss: 0.5586 - val_acc: 0.8527\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 55s 199us/step - loss: 0.3979 - acc: 0.8885 - val_loss: 0.5502 - val_acc: 0.8550\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 54s 194us/step - loss: 0.3599 - acc: 0.8987 - val_loss: 0.5501 - val_acc: 0.8555\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 56s 202us/step - loss: 0.3275 - acc: 0.9074 - val_loss: 0.5537 - val_acc: 0.8573\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 54s 196us/step - loss: 0.2997 - acc: 0.9144 - val_loss: 0.5618 - val_acc: 0.8584\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 60s 214us/step - loss: 0.2756 - acc: 0.9207 - val_loss: 0.5688 - val_acc: 0.8577\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 54s 194us/step - loss: 0.2549 - acc: 0.9257 - val_loss: 0.5784 - val_acc: 0.8580\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 54s 194us/step - loss: 0.2370 - acc: 0.9305 - val_loss: 0.5878 - val_acc: 0.8579\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 55s 196us/step - loss: 0.2217 - acc: 0.9341 - val_loss: 0.5987 - val_acc: 0.8574\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 54s 194us/step - loss: 0.2086 - acc: 0.9373 - val_loss: 0.6137 - val_acc: 0.8563\n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 54s 194us/step - loss: 0.1972 - acc: 0.9401 - val_loss: 0.6246 - val_acc: 0.8564\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 54s 195us/step - loss: 0.1876 - acc: 0.9424 - val_loss: 0.6386 - val_acc: 0.8547\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 55s 196us/step - loss: 0.1793 - acc: 0.9442 - val_loss: 0.6512 - val_acc: 0.8546\n",
      "Epoch 17/50\n",
      "278612/278612 [==============================] - 55s 197us/step - loss: 0.1721 - acc: 0.9458 - val_loss: 0.6648 - val_acc: 0.8537\n",
      "Epoch 18/50\n",
      "278612/278612 [==============================] - 56s 200us/step - loss: 0.1657 - acc: 0.9475 - val_loss: 0.6792 - val_acc: 0.8532\n",
      "Epoch 19/50\n",
      "278612/278612 [==============================] - 54s 195us/step - loss: 0.1606 - acc: 0.9482 - val_loss: 0.6900 - val_acc: 0.8527\n",
      "Epoch 20/50\n",
      "278612/278612 [==============================] - 55s 196us/step - loss: 0.1555 - acc: 0.9493 - val_loss: 0.7035 - val_acc: 0.8522\n",
      "Epoch 21/50\n",
      "278612/278612 [==============================] - 55s 196us/step - loss: 0.1516 - acc: 0.9504 - val_loss: 0.7199 - val_acc: 0.8504\n",
      "Epoch 22/50\n",
      "278612/278612 [==============================] - 55s 196us/step - loss: 0.1480 - acc: 0.9514 - val_loss: 0.7326 - val_acc: 0.8494\n",
      "Epoch 23/50\n",
      "278612/278612 [==============================] - 56s 203us/step - loss: 0.1448 - acc: 0.9521 - val_loss: 0.7443 - val_acc: 0.8486\n",
      "Epoch 24/50\n",
      "278612/278612 [==============================] - 55s 196us/step - loss: 0.1420 - acc: 0.9524 - val_loss: 0.7538 - val_acc: 0.8492\n",
      "Epoch 25/50\n",
      "278612/278612 [==============================] - 55s 196us/step - loss: 0.1399 - acc: 0.9533 - val_loss: 0.7678 - val_acc: 0.8475\n",
      "Epoch 26/50\n",
      "278612/278612 [==============================] - 55s 198us/step - loss: 0.1378 - acc: 0.9536 - val_loss: 0.7799 - val_acc: 0.8470\n",
      "Epoch 27/50\n",
      "278612/278612 [==============================] - 55s 198us/step - loss: 0.1359 - acc: 0.9542 - val_loss: 0.7916 - val_acc: 0.8465\n",
      "Epoch 28/50\n",
      "278612/278612 [==============================] - 55s 198us/step - loss: 0.1340 - acc: 0.9547 - val_loss: 0.8015 - val_acc: 0.8455\n",
      "Epoch 29/50\n",
      "278612/278612 [==============================] - 55s 197us/step - loss: 0.1325 - acc: 0.9548 - val_loss: 0.8183 - val_acc: 0.8441\n",
      "Epoch 30/50\n",
      "278612/278612 [==============================] - 55s 198us/step - loss: 0.1311 - acc: 0.9554 - val_loss: 0.8260 - val_acc: 0.8435\n",
      "Epoch 31/50\n",
      "278612/278612 [==============================] - 55s 199us/step - loss: 0.1303 - acc: 0.9552 - val_loss: 0.8376 - val_acc: 0.8438\n",
      "Epoch 32/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1290 - acc: 0.9557 - val_loss: 0.8476 - val_acc: 0.8435\n",
      "Epoch 33/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1283 - acc: 0.9558 - val_loss: 0.8591 - val_acc: 0.8421\n",
      "Epoch 34/50\n",
      "278612/278612 [==============================] - 50s 179us/step - loss: 0.1274 - acc: 0.9559 - val_loss: 0.8708 - val_acc: 0.8406\n",
      "Epoch 35/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1264 - acc: 0.9563 - val_loss: 0.8806 - val_acc: 0.8403\n",
      "Epoch 36/50\n",
      "278612/278612 [==============================] - 50s 179us/step - loss: 0.1258 - acc: 0.9562 - val_loss: 0.8888 - val_acc: 0.8403\n",
      "Epoch 37/50\n",
      "278612/278612 [==============================] - 51s 181us/step - loss: 0.1254 - acc: 0.9565 - val_loss: 0.8991 - val_acc: 0.8396\n",
      "Epoch 38/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1246 - acc: 0.9568 - val_loss: 0.9148 - val_acc: 0.8374\n",
      "Epoch 39/50\n",
      "278612/278612 [==============================] - 50s 179us/step - loss: 0.1238 - acc: 0.9568 - val_loss: 0.9236 - val_acc: 0.8367\n",
      "Epoch 40/50\n",
      "278612/278612 [==============================] - 51s 182us/step - loss: 0.1234 - acc: 0.9573 - val_loss: 0.9338 - val_acc: 0.8359\n",
      "Epoch 41/50\n",
      "278612/278612 [==============================] - 50s 179us/step - loss: 0.1235 - acc: 0.9570 - val_loss: 0.9436 - val_acc: 0.8354\n",
      "Epoch 42/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1226 - acc: 0.9571 - val_loss: 0.9525 - val_acc: 0.8350\n",
      "Epoch 43/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1226 - acc: 0.9572 - val_loss: 0.9629 - val_acc: 0.8345\n",
      "Epoch 44/50\n",
      "278612/278612 [==============================] - 50s 179us/step - loss: 0.1221 - acc: 0.9570 - val_loss: 0.9727 - val_acc: 0.8333\n",
      "Epoch 45/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1220 - acc: 0.9574 - val_loss: 0.9828 - val_acc: 0.8329\n",
      "Epoch 46/50\n",
      "278612/278612 [==============================] - 50s 181us/step - loss: 0.1217 - acc: 0.9574 - val_loss: 0.9949 - val_acc: 0.8322\n",
      "Epoch 47/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1213 - acc: 0.9576 - val_loss: 1.0035 - val_acc: 0.8309\n",
      "Epoch 48/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1211 - acc: 0.9575 - val_loss: 1.0105 - val_acc: 0.8316\n",
      "Epoch 49/50\n",
      "278612/278612 [==============================] - 50s 180us/step - loss: 0.1211 - acc: 0.9576 - val_loss: 1.0177 - val_acc: 0.8310\n",
      "Epoch 50/50\n",
      "278612/278612 [==============================] - 50s 179us/step - loss: 0.1211 - acc: 0.9576 - val_loss: 1.0272 - val_acc: 0.8299\n"
     ]
    }
   ],
   "source": [
    "history_feedforward = feedforward.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=50, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a simple RNN with 128 dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.add(Embedding(10000, 128, input_length=100))\n",
    "rnn.add(SimpleRNN(128))\n",
    "rnn.add(Dense(24, activation='softmax'))\n",
    "rnn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 1.6117 - acc: 0.5493 - val_loss: 1.0364 - val_acc: 0.7199\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.7364 - acc: 0.8031 - val_loss: 0.8671 - val_acc: 0.7686\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.6146 - acc: 0.8334 - val_loss: 0.8260 - val_acc: 0.7746\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 352s 1ms/step - loss: 0.5473 - acc: 0.8506 - val_loss: 0.7126 - val_acc: 0.8087\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 349s 1ms/step - loss: 0.5003 - acc: 0.8628 - val_loss: 0.7000 - val_acc: 0.8140\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.4623 - acc: 0.8724 - val_loss: 0.7266 - val_acc: 0.8050\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.4301 - acc: 0.8803 - val_loss: 0.6818 - val_acc: 0.8229\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.4023 - acc: 0.8876 - val_loss: 0.7019 - val_acc: 0.8145\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.3775 - acc: 0.8945 - val_loss: 0.6509 - val_acc: 0.8339\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.3550 - acc: 0.8998 - val_loss: 0.7045 - val_acc: 0.8242\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.3371 - acc: 0.9046 - val_loss: 0.7088 - val_acc: 0.8177\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.3190 - acc: 0.9091 - val_loss: 0.6673 - val_acc: 0.8363\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.3039 - acc: 0.9133 - val_loss: 0.6799 - val_acc: 0.8357\n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.2899 - acc: 0.9171 - val_loss: 0.7466 - val_acc: 0.8139\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.2773 - acc: 0.9202 - val_loss: 0.6844 - val_acc: 0.8375\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.2670 - acc: 0.9230 - val_loss: 0.7724 - val_acc: 0.8163\n",
      "Epoch 17/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.2566 - acc: 0.9258 - val_loss: 0.7218 - val_acc: 0.8310\n",
      "Epoch 18/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.2461 - acc: 0.9285 - val_loss: 0.6909 - val_acc: 0.8409\n",
      "Epoch 19/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.2375 - acc: 0.9306 - val_loss: 0.7300 - val_acc: 0.8307\n",
      "Epoch 20/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.2316 - acc: 0.9322 - val_loss: 0.8204 - val_acc: 0.8100\n",
      "Epoch 21/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.2248 - acc: 0.9343 - val_loss: 0.7469 - val_acc: 0.8299\n",
      "Epoch 22/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.2177 - acc: 0.9359 - val_loss: 0.7463 - val_acc: 0.8334\n",
      "Epoch 23/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.2108 - acc: 0.9379 - val_loss: 0.7432 - val_acc: 0.8334\n",
      "Epoch 24/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.2056 - acc: 0.9392 - val_loss: 0.7480 - val_acc: 0.8358\n",
      "Epoch 25/50\n",
      "278612/278612 [==============================] - 348s 1ms/step - loss: 0.2015 - acc: 0.9402 - val_loss: 0.7675 - val_acc: 0.8309\n",
      "Epoch 26/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1955 - acc: 0.9420 - val_loss: 0.7923 - val_acc: 0.8252\n",
      "Epoch 27/50\n",
      "278612/278612 [==============================] - 348s 1ms/step - loss: 0.1932 - acc: 0.9427 - val_loss: 0.7730 - val_acc: 0.8314\n",
      "Epoch 28/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.1885 - acc: 0.9440 - val_loss: 0.7912 - val_acc: 0.8305\n",
      "Epoch 29/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1848 - acc: 0.9447 - val_loss: 0.8017 - val_acc: 0.8289\n",
      "Epoch 30/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1818 - acc: 0.9456 - val_loss: 0.8078 - val_acc: 0.8251\n",
      "Epoch 31/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1792 - acc: 0.9458 - val_loss: 0.7819 - val_acc: 0.8357\n",
      "Epoch 32/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1780 - acc: 0.9461 - val_loss: 0.7974 - val_acc: 0.8313\n",
      "Epoch 33/50\n",
      "278612/278612 [==============================] - 347s 1ms/step - loss: 0.1744 - acc: 0.9472 - val_loss: 0.8137 - val_acc: 0.8284\n",
      "Epoch 34/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1723 - acc: 0.9479 - val_loss: 0.8076 - val_acc: 0.8299\n",
      "Epoch 35/50\n",
      "278612/278612 [==============================] - 348s 1ms/step - loss: 0.1691 - acc: 0.9490 - val_loss: 0.7970 - val_acc: 0.8357\n",
      "Epoch 36/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1674 - acc: 0.9493 - val_loss: 0.8124 - val_acc: 0.8318\n",
      "Epoch 37/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1642 - acc: 0.9496 - val_loss: 0.8185 - val_acc: 0.8329\n",
      "Epoch 38/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1655 - acc: 0.9496 - val_loss: 0.8098 - val_acc: 0.8326\n",
      "Epoch 39/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1628 - acc: 0.9500 - val_loss: 0.8204 - val_acc: 0.8297\n",
      "Epoch 40/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1607 - acc: 0.9505 - val_loss: 0.8388 - val_acc: 0.8311\n",
      "Epoch 41/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1587 - acc: 0.9513 - val_loss: 0.8883 - val_acc: 0.8096\n",
      "Epoch 42/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1597 - acc: 0.9511 - val_loss: 0.8609 - val_acc: 0.8234\n",
      "Epoch 43/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1573 - acc: 0.9518 - val_loss: 0.8233 - val_acc: 0.8334\n",
      "Epoch 44/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1556 - acc: 0.9526 - val_loss: 0.8359 - val_acc: 0.8286\n",
      "Epoch 45/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1542 - acc: 0.9523 - val_loss: 0.8435 - val_acc: 0.8274\n",
      "Epoch 46/50\n",
      "278612/278612 [==============================] - 352s 1ms/step - loss: 0.1543 - acc: 0.9523 - val_loss: 0.8353 - val_acc: 0.8321\n",
      "Epoch 47/50\n",
      "278612/278612 [==============================] - 345s 1ms/step - loss: 0.1534 - acc: 0.9526 - val_loss: 0.8453 - val_acc: 0.8307\n",
      "Epoch 48/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1529 - acc: 0.9528 - val_loss: 0.8506 - val_acc: 0.8287\n",
      "Epoch 49/50\n",
      "278612/278612 [==============================] - 344s 1ms/step - loss: 0.1503 - acc: 0.9536 - val_loss: 0.8672 - val_acc: 0.8240\n",
      "Epoch 50/50\n",
      "278612/278612 [==============================] - 346s 1ms/step - loss: 0.1498 - acc: 0.9538 - val_loss: 0.8630 - val_acc: 0.8239\n"
     ]
    }
   ],
   "source": [
    "history_rnn = rnn.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=50, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM with embeddings of 128 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.add(Embedding(10000, 128, input_length=100))\n",
    "lstm.add(LSTM(128))\n",
    "lstm.add(Dense(24, activation='softmax'))\n",
    "lstm.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 965s 3ms/step - loss: 1.3411 - acc: 0.6276 - val_loss: 0.7728 - val_acc: 0.7892\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 972s 3ms/step - loss: 0.6051 - acc: 0.8318 - val_loss: 0.6071 - val_acc: 0.8273\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 958s 3ms/step - loss: 0.5067 - acc: 0.8537 - val_loss: 0.5440 - val_acc: 0.8446\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 961s 3ms/step - loss: 0.4591 - acc: 0.8653 - val_loss: 0.5148 - val_acc: 0.8521\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 956s 3ms/step - loss: 0.4267 - acc: 0.8728 - val_loss: 0.5106 - val_acc: 0.8523\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 960s 3ms/step - loss: 0.4020 - acc: 0.8797 - val_loss: 0.4928 - val_acc: 0.8573\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 967s 3ms/step - loss: 0.3831 - acc: 0.8853 - val_loss: 0.4826 - val_acc: 0.8610\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 957s 3ms/step - loss: 0.3659 - acc: 0.8892 - val_loss: 0.4813 - val_acc: 0.8633\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 962s 3ms/step - loss: 0.3504 - acc: 0.8939 - val_loss: 0.4830 - val_acc: 0.8638\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 962s 3ms/step - loss: 0.3365 - acc: 0.8977 - val_loss: 0.4844 - val_acc: 0.8646\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 964s 3ms/step - loss: 0.3241 - acc: 0.9009 - val_loss: 0.4837 - val_acc: 0.8648\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 967s 3ms/step - loss: 0.3124 - acc: 0.9046 - val_loss: 0.4833 - val_acc: 0.8666\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 960s 3ms/step - loss: 0.3009 - acc: 0.9079 - val_loss: 0.4934 - val_acc: 0.8639\n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 965s 3ms/step - loss: 0.2900 - acc: 0.9112 - val_loss: 0.4903 - val_acc: 0.8669\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 1003s 4ms/step - loss: 0.2797 - acc: 0.9138 - val_loss: 0.4909 - val_acc: 0.8669\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 1048s 4ms/step - loss: 0.2701 - acc: 0.9168 - val_loss: 0.5005 - val_acc: 0.8664\n",
      "Epoch 17/50\n",
      "235520/278612 [========================>.....] - ETA: 2:33 - loss: 0.2593 - acc: 0.9194"
     ]
    }
   ],
   "source": [
    "history_lstm = lstm.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=50, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU with enbeddings layer of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.add(Embedding(10000, 128, input_length=100))\n",
    "gru.add(GRU(128))\n",
    "gru.add(Dense(24, activation='softmax'))\n",
    "gru.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_gru = gru.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=50, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first custom model we use higher dimensional embedding for a double RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.add(Embedding(10000, 256, input_length=100))\n",
    "model_1.add(SimpleRNN(256, return_sequences=True))\n",
    "model_1.add(SimpleRNN(256))\n",
    "model_1.add(Dense(24, activation='softmax'))\n",
    "model_1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model_1 = model_1.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue using higher dimensions and also add a LSTM cell, and with the ADAM optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.add(Embedding(10000, 256, input_length=100))\n",
    "model_2.add(SimpleRNN(256, return_sequences=True))\n",
    "model_2.add(SimpleRNN(256, return_sequences=True))\n",
    "model_2.add(LSTM(256))\n",
    "model_2.add(Dense(24, activation='softmax'))\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model_2 = model_2.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now experiment with the adagrad optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.add(Embedding(10000, 256, input_length=100))\n",
    "model_3.add(SimpleRNN(256, dropout=0.2))\n",
    "model_3.add(Dense(24, activation='softmax'))\n",
    "model_3.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model_3 = model_3.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use another variation of an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.add(Embedding(10000, 256, input_length=100))\n",
    "model_4.add(LSTM(256, dropout=0.2))\n",
    "model_4.add(Dense(24, activation='softmax'))\n",
    "model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model_4 = lstm_drop.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use GRU with higher dimensional embeddings and ADAGRAD optimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.add(Embedding(10000, 256, input_length=100))\n",
    "model_5.add(GRU(256, dropout=0.2))\n",
    "model_5.add(Dense(24, activation='softmax'))\n",
    "model_5.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_model_5 = model_5.fit(train_x, train_y, validation_data=(valid_x,valid_y), epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ff = history_feedforward.history['val_loss']\n",
    "loss_rnn = history_rnn.history['val_loss']\n",
    "loss_lstm = history_lstm.history['val_loss']\n",
    "loss_gru = history_gru.history['val_loss']\n",
    "loss_model_1 = history_model_1.history['val_loss']\n",
    "loss_model_2 = history_model_2.history['val_loss']\n",
    "loss_model_3 = history_model_3_drop.history['val_loss']\n",
    "loss_model_4 = history_lstm_model_4.history['val_loss']\n",
    "loss_model_5 = history_model_5.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ff = history_feedforward.history['val_acc']\n",
    "acc_rnn = history_rnn.history['val_acc']\n",
    "acc_lstm = history_lstm.history['val_acc']\n",
    "acc_gru = history_gru.history['val_acc']\n",
    "acc_model_1 = history_model_1.history['val_acc']\n",
    "acc_model_2 = history_model_2.history['val_acc']\n",
    "acc_model_3 = history_model_3.history['val_acc']\n",
    "acc_model_4 = history_model_4.history['val_acc']\n",
    "acc_model_5 = history_model_5.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ff)\n",
    "plt.plot(loss_rnn)\n",
    "plt.plot(loss_lstm)\n",
    "plt.plot(loss_gru)\n",
    "plt.plot(loss_model_1)\n",
    "plt.plot(loss_model_2)\n",
    "plt.plot(loss_model_3)\n",
    "plt.plot(loss_model_4)\n",
    "plt.plot(loss_model_5)\n",
    "plt.legend(['ff', 'rnn', 'lstm', 'gru', 'model_1', 'model_2', 'model_3', 'model_4', 'model_5'])\n",
    "plt.title('The validation loss of the models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_ff)\n",
    "plt.plot(acc_rnn)\n",
    "plt.plot(acc_lstm)\n",
    "plt.plot(acc_gru)\n",
    "plt.plot(acc_model_1)\n",
    "plt.plot(acc_model_2)\n",
    "plt.plot(acc_model_3)\n",
    "plt.plot(acc_model_4)\n",
    "plt.plot(acc_model_5)\n",
    "plt.legend(['ff', 'rnn', 'lstm', 'gru', 'model_1', 'model_2', 'model_3', 'model_4', 'model_5'])\n",
    "plt.title('The validation accuracy models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.evaluate(test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
